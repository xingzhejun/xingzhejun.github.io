---
---

@article{luo2025sample,
  title={Sample By Step, Optimize By Chunk: Chunk-Level GRPO For Text-to-Image Generation},
  author={Luo, Yifu and Du, Penghui and Li, Bo and Du, Sinan and Zhang, Tiantian and Chang, Yongzhe and Wu, Kai and Gai, Kun and Wang, Xueqian},
  journal={arXiv preprint arXiv:2510.21583},
  year={2025},
  url={https://arxiv.org/abs/2510.21583},
  abstract={Group Relative Policy Optimization (GRPO) has shown strong potential for flow-matching-based text-to-image (T2I) generation, but it faces two key limitations: inaccurate advantage attribution, and the neglect of temporal dynamics of generation. In this work, we argue that shifting the optimization paradigm from the step level to the chunk level can effectively alleviate these issues. Building on this idea, we propose Chunk-GRPO, the first chunk-level GRPO-based approach for T2I generation. The insight is to group consecutive steps into coherent 'chunk's that capture the intrinsic temporal dynamics of flow matching, and to optimize policies at the chunk level. In addition, we introduce an optional weighted sampling strategy to further enhance performance. Extensive experiments show that ChunkGRPO achieves superior results in both preference alignment and image quality, highlighting the promise of chunk-level optimization for GRPO-based methods.},
  selected={true}
}

@inproceedings{luoreinforcement,
  title={Reinforcement Learning Meets Masked Generative Models: Mask-GRPO for Text-to-Image Generation},
  author={Luo, Yifu and Hu, Xinhao and Fan, Keyu and Sun, Haoyuan and Chen, Zeyu and Xia, Bo and Zhang, Tiantian and Chang, Yongzhe and Wang, Xueqian},
  booktitle={The Thirty-ninth Annual Conference on Neural Information Processing Systems},
  url={https://arxiv.org/abs/2510.13418},
  abstract={Reinforcement learning (RL) has garnered increasing attention in text-to-image (T2I) generation. However, most existing RL approaches are tailored to either diffusion models or autoregressive models, overlooking an important alternative: masked generative models. In this work, we propose Mask-GRPO, the first method to incorporate Group Relative Policy Optimization (GRPO)-based RL into this overlooked paradigm. Our core insight is to redefine the transition probability, which is different from current approaches, and formulate the unmasking process as a multi-step decision-making problem. To further enhance our method, we explore several useful strategies, including removing the KL constraint, applying the reduction strategy, and filtering out low-quality samples. Using Mask-GRPO, we improve a base model, Show-o, with substantial improvements on standard T2I benchmarks and preference alignment, outperforming existing state-of-the-art approaches.},
  selected={true}
}

@article{sun2025reinforcement,
  title={Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models},
  author={Sun, Haoyuan and Wu, Jiaqi and Xia, Bo and Luo, Yifu and Zhao, Yifei and Qin, Kai and Lv, Xufei and Zhang, Tiantian and Chang, Yongzhe and Wang, Xueqian},
  journal={arXiv preprint arXiv:2505.18536},
  year={2025},
  url={https://arxiv.org/abs/2505.18536}
}

@article{luo2025wavelet,
  title={Wavelet Fourier Diffuser: Frequency-Aware Diffusion Model for Reinforcement Learning},
  author={Luo, Yifu and Chang, Yongzhe and Wang, Xueqian},
  journal={arXiv preprint arXiv:2509.19305},
  year={2025},
  url={https://arxiv.org/abs/2509.19305}
}

@inproceedings{xia2024d3d,
  title={D3D: Conditional Diffusion Model for Decision-Making Under Random Frame Dropping},
  author={Xia, Bo and Luo, Yifu and Chang, Yongzhe and Yuan, Bo and Li, Zhiheng and Wang, Xueqian},
  booktitle={2024 33rd IEEE International Conference on Robot and Human Interactive Communication (ROMAN)},
  pages={278--284},
  year={2024},
  organization={IEEE}
}
